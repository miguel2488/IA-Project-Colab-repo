{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libraries imported!!\n"
     ]
    }
   ],
   "source": [
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.viewer import ImageViewer\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import misc\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print('libraries imported!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bauer\\OneDrive para la Empresa\\Microsoft Capstone IA\n",
      "988\n",
      "988\n",
      "659\n",
      "659\n",
      "988\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "train_img_dir = os.getcwd() + '/Capstone data/train/'\n",
    "test_img_dir = os.getcwd() + '/Capstone data/test/'\n",
    "\n",
    "#spectrogram of current \n",
    "train_img_names_c = [x for x in os.listdir(train_img_dir) if x.endswith('_c.png')]\n",
    "\n",
    "#spectrogram of voltage\n",
    "train_img_names_v = [x for x in os.listdir(train_img_dir) if x.endswith('_v.png')]\n",
    "\n",
    "#spectrogram of current \n",
    "test_img_names_c = [x for x in os.listdir(test_img_dir) if x.endswith('_c.png')]\n",
    "\n",
    "#spectrogram of voltage\n",
    "test_img_names_v = [x for x in os.listdir(test_img_dir) if x.endswith('_v.png')]\n",
    "\n",
    "\n",
    "print(len(train_img_names_c))\n",
    "print(len(train_img_names_v))\n",
    "\n",
    "print(len(test_img_names_c))\n",
    "print(len(test_img_names_v))\n",
    "print(len(train_img_names_c))\n",
    "\n",
    "all_train_images = np.ones((len(train_img_names_c), 128, 118)) #if as_grey=True\n",
    "all_test_images = np.ones((len(test_img_names_c), 128, 118)) #if as_grey=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_train_images.shape (988, 128, 118)\n",
      "all_test_images.shape (659, 128, 118)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for im in train_img_names_c:\n",
    "\n",
    "    # Load an color image in grayscale\n",
    "    my_image = cv2.imread(train_img_dir + '/' + im,0)\n",
    "\n",
    "\n",
    "    all_train_images[i] = my_image\n",
    "    i = i + 1\n",
    "\n",
    "    \n",
    "i = 0\n",
    "for im in test_img_names_c:\n",
    "    #print(im)\n",
    "    my_image = cv2.imread(test_img_dir + '/' + im,0)\n",
    "\n",
    "\n",
    "    all_test_images[i] = my_image\n",
    "    i = i + 1    \n",
    "\n",
    "\n",
    "print(\"all_train_images.shape {0}\".format(all_train_images.shape))\n",
    "print(\"all_test_images.shape {0}\".format(all_test_images.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 30.  30.  30. ...  30.  30.  30.]\n",
      " [ 30.  30.  30. ...  30.  30.  30.]\n",
      " [ 30.  30.  30. ...  30.  30.  30.]\n",
      " ...\n",
      " [ 77.  73.  71. ... 194. 195. 200.]\n",
      " [ 78.  81.  82. ... 212. 212. 205.]\n",
      " [ 69.  63.  62. ... 179. 181. 185.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 117.5, 127.5, -0.5)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# look at the image\n",
    "#plt.imshow(my_image)\n",
    "\n",
    "\n",
    "\n",
    "sample_number = 12\n",
    "print(all_train_images[sample_number])\n",
    "plt.imshow(all_train_images[sample_number], cmap=\"gray_r\")\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************\n",
      "all_train_images.shape (988, 128, 118)\n",
      "data_train shape (988, 15104)\n",
      "[[ 38.  47.  51. ... 179. 184. 197.]\n",
      " [ 30.  30.  30. ... 177. 183. 196.]\n",
      " [ 62.  61.  64. ... 180. 187. 204.]\n",
      " ...\n",
      " [ 30.  30.  30. ... 180. 186. 200.]\n",
      " [ 67.  68.  66. ... 152. 159. 179.]\n",
      " [ 74.  61.  46. ... 153. 165. 159.]]\n",
      "**************************\n",
      "\n",
      "\n",
      "**************************\n",
      "all_test_images.shape (659, 128, 118)\n",
      "data_test shape (659, 15104)\n",
      "[[ 84.  81.  77. ... 179. 179. 172.]\n",
      " [ 30.  30.  30. ... 180. 181. 180.]\n",
      " [ 30.  30.  30. ... 152. 153. 148.]\n",
      " ...\n",
      " [ 56.  61.  62. ... 180. 180. 174.]\n",
      " [ 30.  30.  30. ... 179. 188. 205.]\n",
      " [ 70.  71.  69. ... 152. 154. 162.]]\n",
      "**************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "\n",
    "print(\"\\n**************************\")\n",
    "n_samples = len(all_train_images)\n",
    "\n",
    "print(\"all_train_images.shape {0}\".format(all_train_images.shape))\n",
    "data_train = all_train_images.reshape((n_samples, 15104)) #988, 15104, 4\n",
    "\n",
    "print(\"data_train shape {0}\".format(data_train.shape))\n",
    "print(data_train[:10])\n",
    "\n",
    "print(\"**************************\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n**************************\")\n",
    "n_samples = len(all_test_images)\n",
    "\n",
    "print(\"all_test_images.shape {0}\".format(all_test_images.shape))\n",
    "data_test = all_test_images.reshape((n_samples, 15104)) #988, 15104, 4\n",
    "\n",
    "print(\"data_test shape {0}\".format(data_test.shape))\n",
    "print(data_test[:10])\n",
    "\n",
    "print(\"**************************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988\n",
      "(988, 2)\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "\n",
    "\n",
    "train_labels_dir = os.getcwd() + '/'\n",
    "\n",
    "train_labels = genfromtxt(train_labels_dir + 'train_labels.csv', delimiter=',', skip_header=1)\n",
    "\n",
    "print(len(train_labels))\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data and prepare the text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_labels shape is : (988, 2) \n",
      "\n",
      "x_train shape is:  (691, 15104) \n",
      " y_train shape is:  (691, 2) \n",
      "\n",
      "x_test shape is:  (297, 15104) \n",
      " y_test shape is:  (297, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(data_train, train_labels, test_size = 0.3, random_state = 54)\n",
    "\n",
    "print('train_labels shape is :', train_labels.shape, '\\n')\n",
    "print('x_train shape is: ', x_train.shape, '\\n',\n",
    "      'y_train shape is: ', y_train.shape, '\\n')\n",
    "\n",
    "print('x_test shape is: ', x_test.shape, '\\n', \n",
    "      'y_test shape is: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data files into a format compatible with CNTK text reader\n",
    "def savetxt(filename, data, hasLabels=True, labels=0):\n",
    "    dir = os.path.dirname(filename)\n",
    "\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    \n",
    "    print(\"Saving\", filename )\n",
    "    with open(filename, 'w') as f:\n",
    "        print(\"opened....\")\n",
    "        labels_ohe = list(map(' '.join, np.eye(11, dtype=np.uint).astype(str))) #for one hot encoding\n",
    "        index = 0\n",
    "        for row in data:            \n",
    "            row_str = row.astype(str)\n",
    "            if hasLabels:                               \n",
    "                label_str = labels_ohe[int(labels[index])]               \n",
    "            \n",
    "            feature_str = ' '.join(row_str)\n",
    "            \n",
    "            if hasLabels:\n",
    "                f.write('|labels {} |features {}\\n'.format(label_str, feature_str))\n",
    "            else:\n",
    "                f.write('|features {}\\n'.format(feature_str))\n",
    "            \n",
    "            index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train text file...\n",
      "Saving C:\\Users\\bauer\\OneDrive para la Empresa\\Microsoft Capstone IA\\data/Out\\train.txt\n",
      "opened....\n",
      "Saving C:\\Users\\bauer\\OneDrive para la Empresa\\Microsoft Capstone IA\\data/Out\\test.txt\n",
      "opened....\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_labels_GT = y_train[:,1] #Get Ground truth\n",
    "test_labels_GT = y_test[:,1]\n",
    "\n",
    "print ('Writing train text file...')\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), \"data/Out\")\n",
    "\n",
    "\n",
    "savetxt(os.path.join(data_dir, \"train.txt\"), x_train, True, train_labels_GT)\n",
    "savetxt(os.path.join(data_dir, \"test.txt\"), x_test, True, test_labels_GT)\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Reader Deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a CTF formatted text (as mentioned above) using the CTF deserializer from a file\n",
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    \n",
    "    labelStream = C.io.StreamDef(field='labels', shape=num_label_classes, is_sparse=False)\n",
    "    featureStream = C.io.StreamDef(field='features', shape=input_dim, is_sparse=False)\n",
    "    \n",
    "    deserailizer = C.io.CTFDeserializer(path, C.io.StreamDefs(labels = labelStream, features = featureStream))\n",
    "            \n",
    "    return C.io.MinibatchSource(deserailizer,\n",
    "       randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cntk as C\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Ensure we always get the same amount of randomness\n",
    "np.random.seed(0)\n",
    "C.cntk_py.set_fixed_random_seed(1)\n",
    "C.cntk_py.force_deterministic_algorithms()\n",
    "\n",
    "# Define the data dimensions\n",
    "input_dim_model = (1, 128, 118)    # images are 28 x 28 with 1 channel of color (gray)\n",
    "input_dim = 128*118                # used by readers to treat input data as a vector\n",
    "num_output_classes = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = C.input_variable(input_dim_model)\n",
    "y = C.input_variable(num_output_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape of the first convolution layer: (8, 64, 59)\n",
      "Bias value of the last dense layer: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def create_model(features):\n",
    "    with C.layers.default_options(init=C.glorot_uniform(), activation=C.relu):\n",
    "            h = features\n",
    "            h = C.layers.Convolution2D(filter_shape=(5,5), \n",
    "                                       num_filters=8, \n",
    "                                       strides=(2,2), \n",
    "                                       pad=True, name='first_conv')(h)\n",
    "            h = C.layers.Convolution2D(filter_shape=(5,5), \n",
    "                                       num_filters=16, \n",
    "                                       strides=(2,2), \n",
    "                                       pad=True, name='second_conv')(h)\n",
    "            r = C.layers.Dense(num_output_classes, activation= C.relu, name='classify')(h)\n",
    "            return r\n",
    "        \n",
    "# Create the model\n",
    "z = create_model(x)\n",
    "\n",
    "# Print the output shapes / parameters of different components\n",
    "print(\"Output Shape of the first convolution layer:\", z.first_conv.shape)\n",
    "print(\"Bias value of the last dense layer:\", z.classify.b.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 172395 parameters in 6 parameter tensors.\n"
     ]
    }
   ],
   "source": [
    "# Number of parameters in the network\n",
    "C.logging.log_number_of_parameters(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_criterion_function(model, labels):\n",
    "    loss = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error(model, labels)\n",
    "    return loss, errs # (model, labels) -> (loss, error metric)\n",
    "\n",
    "# Define a utility function to compute the moving average sum.\n",
    "# A more efficient implementation is possible with np.cumsum() function\n",
    "def moving_average(a, w=5):\n",
    "    if len(a) < w:\n",
    "        return a[:]    # Need to send a copy of the array\n",
    "    return [val if idx < w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]\n",
    "\n",
    "\n",
    "# Defines a utility that prints the training progress\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):\n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "\n",
    "    if mb%frequency == 0:\n",
    "        training_loss = trainer.previous_minibatch_loss_average\n",
    "        eval_error = trainer.previous_minibatch_evaluation_average\n",
    "        if verbose: \n",
    "            print (\"Minibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}%\".format(mb, training_loss, eval_error*100))\n",
    "        \n",
    "    return mb, training_loss, eval_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train_test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test(train_reader, test_reader, model_func, num_sweeps_to_train_with=10):\n",
    "    \n",
    "    # Instantiate the model function; x is the input (feature) variable \n",
    "    # We will scale the input image pixels within 0-1 range by dividing all input value by 255.\n",
    "    model = model_func(x/255)\n",
    "    \n",
    "    # Instantiate the loss and error function\n",
    "    loss, label_error = create_criterion_function(model, y)\n",
    "    \n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    learning_rate = 0.2\n",
    "    lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)\n",
    "    learner = C.sgd(z.parameters, lr_schedule)\n",
    "    trainer = C.Trainer(z, (loss, label_error), [learner])\n",
    "    \n",
    "    # Initialize the parameters for the trainer\n",
    "    minibatch_size = 1\n",
    "    num_samples_per_sweep = 691\n",
    "    num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) / minibatch_size\n",
    "    \n",
    "    # Map the data streams to the input and labels.\n",
    "    input_map={\n",
    "        y  : train_reader.streams.labels,\n",
    "        x  : train_reader.streams.features\n",
    "    } \n",
    "    \n",
    "    # Uncomment below for more detailed logging\n",
    "    training_progress_output_freq = 10\n",
    "     \n",
    "    # Start a timer\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(0, int(num_minibatches_to_train)):\n",
    "        # Read a mini batch from the training data file\n",
    "        data=train_reader.next_minibatch(minibatch_size, input_map=input_map) \n",
    "        trainer.train_minibatch(data)\n",
    "        print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\n",
    "     \n",
    "    # Print training time\n",
    "    print(\"Training took {:.1f} sec\".format(time.time() - start))\n",
    "    \n",
    "    # Test the model\n",
    "    test_input_map = {\n",
    "        y  : test_reader.streams.labels,\n",
    "        x  : test_reader.streams.features\n",
    "    }\n",
    "\n",
    "    # Test data for trained model\n",
    "    test_minibatch_size = 1\n",
    "    num_samples = 297\n",
    "    num_minibatches_to_test = num_samples // test_minibatch_size\n",
    "\n",
    "    test_result = 0.0   \n",
    "\n",
    "    for i in range(num_minibatches_to_test):\n",
    "    \n",
    "        # We are loading test data in batches specified by test_minibatch_size\n",
    "        # Each data point in the minibatch is a MNIST digit image of 784 dimensions \n",
    "        # with one pixel per dimension that we will encode / decode with the \n",
    "        # trained model.\n",
    "        data = test_reader.next_minibatch(test_minibatch_size, input_map=test_input_map)\n",
    "        eval_error = trainer.test_minibatch(data)\n",
    "        test_result = test_result + eval_error\n",
    "\n",
    "    # Average of evaluation errors of all test minibatches\n",
    "    print(\"Average test error: {0:.2f}%\".format(test_result*100 / num_minibatches_to_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'data/out/train.txt'\n",
    "test_file = 'data/out/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch: 0, Loss: 2.4355, Error: 100.00%\n",
      "Minibatch: 10, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 20, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 30, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 40, Loss: 2.4043, Error: 100.00%\n",
      "Minibatch: 50, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 60, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 70, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 80, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 90, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 100, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 110, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 120, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 130, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 140, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 150, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 160, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 170, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 180, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 190, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 200, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 210, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 220, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 230, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 240, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 250, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 260, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 270, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 280, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 290, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 300, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 310, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 320, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 330, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 340, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 350, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 360, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 370, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 380, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 390, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 400, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 410, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 420, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 430, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 440, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 450, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 460, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 470, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 480, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 490, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 500, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 510, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 520, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 530, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 540, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 550, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 560, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 570, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 580, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 590, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 600, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 610, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 620, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 630, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 640, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 650, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 660, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 670, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 680, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 690, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 700, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 710, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 720, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 730, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 740, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 750, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 760, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 770, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 780, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 790, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 800, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 810, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 820, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 830, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 840, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 850, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 860, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 870, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 880, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 890, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 900, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 910, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 920, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 930, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 940, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 950, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 960, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 970, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 980, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 990, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 1000, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1010, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1020, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1030, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1040, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1050, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1060, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1070, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1080, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1090, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1100, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1110, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1120, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1130, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1140, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1150, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1160, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1170, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 1180, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1190, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1200, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1210, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1220, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 1230, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1240, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1250, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1260, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1270, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 1280, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1290, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1300, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1310, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1320, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1330, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1340, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1350, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1360, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1370, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1380, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1390, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 1400, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1410, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1420, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1430, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 1440, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1450, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1460, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1470, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1480, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1490, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1500, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1510, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1520, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1530, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1540, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1550, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1560, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1570, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1580, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1590, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1600, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1610, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1620, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1630, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1640, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1650, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1660, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1670, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1680, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1690, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1700, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1710, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1720, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1730, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1740, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1750, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1760, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1770, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1780, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 1790, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1800, Loss: 2.3979, Error: 100.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch: 1810, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1820, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1830, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1840, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1850, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1860, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1870, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1880, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1890, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1900, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1910, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 1920, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1930, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1940, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 1950, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 1960, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1970, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1980, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 1990, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2000, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2010, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2020, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2030, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2040, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 2050, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 2060, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2070, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2080, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2090, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2100, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2110, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2120, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2130, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2140, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2150, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2160, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2170, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2180, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2190, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2200, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2210, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 2220, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2230, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2240, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2250, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2260, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2270, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2280, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2290, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2300, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2310, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2320, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2330, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2340, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2350, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2360, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 2370, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2380, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 2390, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2400, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2410, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2420, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2430, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2440, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2450, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2460, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2470, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2480, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2490, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2500, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 2510, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2520, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2530, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2540, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2550, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2560, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2570, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2580, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2590, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 2600, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2610, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2620, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2630, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2640, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2650, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2660, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2670, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2680, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2690, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2700, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2710, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2720, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2730, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2740, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2750, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2760, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2770, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2780, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2790, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2800, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2810, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2820, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2830, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2840, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2850, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2860, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2870, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2880, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 2890, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 2900, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 2910, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2920, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2930, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2940, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 2950, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2960, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2970, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2980, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 2990, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3000, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 3010, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3020, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3030, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3040, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3050, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3060, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3070, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3080, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3090, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3100, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 3110, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 3120, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3130, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3140, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3150, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3160, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3170, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3180, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3190, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3200, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3210, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3220, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3230, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3240, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3250, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3260, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3270, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3280, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3290, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3300, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3310, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3320, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3330, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3340, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3350, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3360, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3370, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3380, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 3390, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3400, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3410, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3420, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3430, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3440, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3450, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 3460, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3470, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3480, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3490, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3500, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3510, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3520, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3530, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3540, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3550, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 3560, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3570, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 3580, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3590, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3600, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3610, Loss: 2.3979, Error: 100.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch: 3620, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3630, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3640, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3650, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3660, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3670, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3680, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3690, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3700, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3710, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3720, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3730, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3740, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3750, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3760, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3770, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3780, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3790, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3800, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 3810, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3820, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3830, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3840, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3850, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3860, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3870, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 3880, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3890, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3900, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3910, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3920, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3930, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3940, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3950, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3960, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3970, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3980, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 3990, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4000, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4010, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4020, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4030, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4040, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 4050, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4060, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4070, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4080, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4090, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4100, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4110, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4120, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4130, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4140, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4150, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4160, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4170, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4180, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4190, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4200, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4210, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4220, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4230, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4240, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4250, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4260, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4270, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4280, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4290, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4300, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4310, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4320, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 4330, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4340, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4350, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4360, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4370, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4380, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4390, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4400, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4410, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4420, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 4430, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4440, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4450, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4460, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4470, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 4480, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 4490, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4500, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 4510, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4520, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4530, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4540, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4550, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4560, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4570, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4580, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4590, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4600, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4610, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4620, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4630, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4640, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4650, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4660, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4670, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4680, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4690, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4700, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4710, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4720, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4730, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4740, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 4750, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4760, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4770, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4780, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4790, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4800, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4810, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4820, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4830, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4840, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4850, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4860, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4870, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4880, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4890, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4900, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4910, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4920, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4930, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 4940, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4950, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4960, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4970, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 4980, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 4990, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 5000, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5010, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5020, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 5030, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5040, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5050, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5060, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5070, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5080, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5090, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5100, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 5110, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 5120, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5130, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5140, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5150, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5160, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5170, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5180, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5190, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5200, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5210, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5220, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5230, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5240, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5250, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5260, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5270, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5280, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5290, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 5300, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5310, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5320, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 5330, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5340, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5350, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5360, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5370, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5380, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5390, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5400, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5410, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5420, Loss: 2.3979, Error: 100.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch: 5430, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5440, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5450, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5460, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5470, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5480, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5490, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5500, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5510, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5520, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5530, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5540, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 5550, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5560, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5570, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5580, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5590, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5600, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5610, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 5620, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5630, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5640, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5650, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5660, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5670, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5680, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5690, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5700, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5710, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5720, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5730, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5740, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5750, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5760, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5770, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5780, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 5790, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5800, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5810, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5820, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5830, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5840, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5850, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5860, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 5870, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5880, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5890, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5900, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5910, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5920, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5930, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 5940, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5950, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5960, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5970, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5980, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 5990, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6000, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6010, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6020, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6030, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6040, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6050, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6060, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6070, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6080, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6090, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6100, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6110, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6120, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6130, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6140, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6150, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6160, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6170, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6180, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6190, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6200, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6210, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6220, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6230, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6240, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6250, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6260, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6270, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6280, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6290, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 6300, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 6310, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6320, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6330, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6340, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6350, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 6360, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6370, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 6380, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6390, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6400, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6410, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6420, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6430, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6440, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6450, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6460, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6470, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6480, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6490, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6500, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6510, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6520, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6530, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6540, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6550, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6560, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6570, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6580, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6590, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6600, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6610, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6620, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6630, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6640, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6650, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6660, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6670, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6680, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6690, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6700, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6710, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6720, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6730, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6740, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6750, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6760, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6770, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6780, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6790, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6800, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6810, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6820, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6830, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6840, Loss: 2.3979, Error: 0.00%\n",
      "Minibatch: 6850, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6860, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6870, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6880, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6890, Loss: 2.3979, Error: 100.00%\n",
      "Minibatch: 6900, Loss: 2.3979, Error: 100.00%\n",
      "Training took 32.6 sec\n",
      "Average test error: 85.86%\n",
      "Bias value of the last dense layer: [-0.01762655 -0.01915903  0.12948216 -0.03743833 -0.03629963 -0.01848884\n",
      " -0.0181843  -0.01820344 -0.018018   -0.01825852 -0.0191818 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def do_train_test():\n",
    "    global z\n",
    "    z = create_model(x)\n",
    "    reader_train = create_reader(train_file, True, input_dim, num_output_classes)\n",
    "    reader_test = create_reader(test_file, False, input_dim, num_output_classes)\n",
    "    train_test(reader_train, reader_test, z)\n",
    "    \n",
    "do_train_test()\n",
    "\n",
    "\n",
    "print(\"Bias value of the last dense layer:\", z.classify.b.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = C.softmax(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the save_text function to avoid malformed input file error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data files into a format compatible with CNTK text reader\n",
    "def savetxt(filename, data, hasLabels=True, labels=0):\n",
    "    dir = os.path.dirname(filename)\n",
    "\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    \n",
    "    print(\"Saving\", filename )\n",
    "    with open(filename, 'w') as f:\n",
    "        print(\"opened....\")\n",
    "        labels_ohe = list(map(' '.join, np.eye(11, dtype=np.uint).astype(str))) #for one hot encoding\n",
    "        index = 0\n",
    "        for row in data:            \n",
    "            row_str = row.astype(str)\n",
    "            if hasLabels:                               \n",
    "                label_str = labels_ohe[int(labels[index])]               \n",
    "            \n",
    "            feature_str = ' '.join(row_str)\n",
    "            \n",
    "            if hasLabels:\n",
    "                f.write('|labels {} |features {}\\n'.format(label_str, feature_str))\n",
    "            else:\n",
    "                f.write('|labels {} |features {}\\n'.format(labels_ohe[1], feature_str))\n",
    "\n",
    "            \n",
    "            index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model evaluation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train text file...\n",
      "Saving C:\\Users\\bauer\\OneDrive para la Empresa\\Microsoft Capstone IA\\data/Out\\test_eval.txt\n",
      "opened....\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print ('Writing train text file...')\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), \"data/Out\")\n",
    "\n",
    "\n",
    "savetxt(os.path.join(data_dir, \"test_eval.txt\"), data_test, False)\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data for evaluation\n",
    "eval_file = 'C:/Users/bauer/OneDrive para la Empresa/Microsoft Capstone IA/data/Out/test_eval.txt'\n",
    "reader_eval=create_reader(eval_file, False, input_dim, num_output_classes)\n",
    "\n",
    "eval_minibatch_size = 25\n",
    "eval_input_map = {x: reader_eval.streams.features, y:reader_eval.streams.labels} \n",
    "\n",
    "data = reader_eval.next_minibatch(eval_minibatch_size, input_map=eval_input_map)\n",
    "\n",
    "img_label = data[y].asarray()\n",
    "img_data = data[x].asarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label    : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Predicted: [2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# reshape img_data to: M x 1 x 128 x 118 to be compatible with model\n",
    "img_data = np.reshape(img_data, (eval_minibatch_size, 1, 128, 118))\n",
    "\n",
    "predicted_label_prob = [out.eval(img_data[i]) for i in range(len(img_data))]\n",
    "\n",
    "\n",
    "# Find the index with the maximum value for both predicted as well as the ground truth\n",
    "pred = [np.argmax(predicted_label_prob[i]) for i in range(len(predicted_label_prob))]\n",
    "gtlabel = [np.argmax(img_label[i]) for i in range(len(img_label))]\n",
    "\n",
    "\n",
    "print(\"Label    :\", gtlabel[:25])\n",
    "print(\"Predicted:\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optionally modify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from cntk.layers.layers import AveragePooling\n",
    "\n",
    "def create_model(features):\n",
    "    with C.layers.default_options(init = C.glorot_uniform(), activation = C.relu):\n",
    "            h = features\n",
    "            \n",
    "            h = C.layers.Convolution2D(filter_shape=(5,5), \n",
    "                                       num_filters=8, \n",
    "                                       strides=(1,1), \n",
    "                                       pad=True, name='first_conv')(h)\n",
    "            \n",
    "            p =  AveragePooling((3,3), strides=1)\n",
    "            \n",
    "            ph = p(h)\n",
    "            \n",
    "            h = C.layers.Convolution2D(filter_shape=(5,5), \n",
    "                                       num_filters=16, \n",
    "                                       strides=(1,1), \n",
    "                                       pad=True, name='second_conv')(h)\n",
    "            \n",
    "            p =  AveragePooling((3,3), strides=1)\n",
    "            \n",
    "            ph = p(h)\n",
    "            \n",
    "            r = C.layers.Dense(num_output_classes, activation = None, name='classify')(h)\n",
    "            return r'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally train the model on the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Save the data files into a format compatible with CNTK text reader\n",
    "def savetxt(filename, data, hasLabels=True, labels=0):\n",
    "    dir = os.path.dirname(filename)\n",
    "\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    \n",
    "    print(\"Saving\", filename )\n",
    "    with open(filename, 'w') as f:\n",
    "        print(\"opened....\")\n",
    "        labels_ohe = list(map(' '.join, np.eye(11, dtype=np.uint).astype(str))) #for one hot encoding\n",
    "        index = 0\n",
    "        for row in data:            \n",
    "            row_str = row.astype(str)\n",
    "            if hasLabels:                               \n",
    "                label_str = labels_ohe[int(labels[index])]               \n",
    "            \n",
    "            feature_str = ' '.join(row_str)\n",
    "            \n",
    "            if hasLabels:\n",
    "                f.write('|labels {} |features {}\\n'.format(label_str, feature_str))\n",
    "            else:\n",
    "                f.write('|labels {} |features {}\\n'.format(labels_ohe[1], feature_str))\n",
    "\n",
    "            \n",
    "            index = index + 1\n",
    "\n",
    "\n",
    "\n",
    "train_labels_GT = train_labels[:,1] #Get Ground truth\n",
    "\n",
    "\n",
    "print ('Writing train text file...')\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), \"data/Out\")\n",
    "\n",
    "\n",
    "savetxt(os.path.join(data_dir, \"train.txt\"), data_train, True, train_labels_GT)\n",
    "savetxt(os.path.join(data_dir, \"test.txt\"), data_test, False)\n",
    "\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "do_train_test()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally make predictions again from the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# reshape img_data to: M x 1 x 128 x 118 to be compatible with model\n",
    "img_data = np.reshape(img_data, (eval_minibatch_size, 1, 128, 118))\n",
    "\n",
    "predicted_label_prob = [out.eval(img_data[i]) for i in range(len(img_data))]\n",
    "\n",
    "\n",
    "# Find the index with the maximum value for both predicted as well as the ground truth\n",
    "pred = [np.argmax(predicted_label_prob[i]) for i in range(len(predicted_label_prob))]\n",
    "gtlabel = [np.argmax(img_label[i]) for i in range(len(img_label))]\n",
    "\n",
    "\n",
    "print(\"Label    :\", gtlabel[:25])\n",
    "print(\"Predicted:\", pred)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "id_labels = pd.read_csv('test_labels.csv')\n",
    "predictions = pd.DataFrame(data = pred, columns = ['appliance'])\n",
    "predictions['id'] = id_labels['names']\n",
    "predictions = predictions[['id', 'appliance']]\n",
    "\n",
    "\n",
    "#SAVE CSV\n",
    "predictions.to_csv('Caps_Preds.csv', index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
